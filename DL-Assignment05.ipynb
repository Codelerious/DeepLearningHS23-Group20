{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5: Classification in PyTorch \n",
    "\n",
    "\n",
    "For this exercise, we will switch to an implementation in PyTorch. \n",
    "The goal of this exercise is to get used to some concepts in PyTorch, such as relying on the `torch.tensor` data structure, implementing the network, the loss functions, the training loop and accuracy computation, which we will apply to binary and categorical classification.\n",
    "\n",
    "Please make sure that all your variables are compatible with `torch`.\n",
    "For example, you cannot mix `torch.tensor`s and `numpy.ndarray`s in any part of the code.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We will use two different datasets, the *spambase* dataset https://archive.ics.uci.edu/ml/datasets/spambase for binary classification and the *wine* dataset https://archive.ics.uci.edu/ml/datasets/wine for categorical classification. Both datasets are available on the UCI Machine Learning repository. \n",
    "The binary classification dataset contains features extracted from emails, which are classified as either spam or not. \n",
    "The categorical classification dataset contains some manually selected features for three different types of wines. \n",
    "For the former, the class is provided in the last column of the data file, whereas for the latter, the first index provides class information.\n",
    "\n",
    "Please run the code block below to download the data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded datafile spambase.data\n",
      "Downloaded datafile wine.data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# download the two dataset files\n",
    "dataset_files = {\n",
    "  \"spambase.data\": \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/\",\n",
    "  \"wine.data\": \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/\"\n",
    "}\n",
    "for name, url in dataset_files.items():\n",
    "  if not os.path.exists(name):\n",
    "    import urllib.request\n",
    "    urllib.request.urlretrieve(url+name, name)\n",
    "    print (\"Downloaded datafile\", name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Dataset Loading\n",
    "\n",
    "The first task deals with the loading of the datasets. \n",
    "When training networks in PyTorch, all data needs to be stored as datatype ``torch.tensor``. \n",
    "The data should be split between input sets $\\mathbf X = [\\vec x^{[1]}, \\ldots, \\vec x^{[N]}]^T \\in \\mathbb R^{N\\times D}$ and targets.\n",
    "There is **no need to add a bias neuron to the input**, and the transposition of the data matrix is different from what we have seen before.\n",
    "\n",
    "For the targets, we have to be more careful as there are differences with respect to the applied loss function.\n",
    "For binary classification, we need $\\mathbf T = [[t^{[1]}, \\ldots, t^{[N]}]]$ to be in dimension $\\mathbb R^{N\\times1}$ and of type ``torch.float``.\n",
    "For categorical classification, we only need the class indexes $\\vec t = [t^{[1]}, \\ldots, t^{[N]}]$ to be in dimension $\\mathbb N^N$ and of type ``torch.long``.\n",
    "\n",
    "Implement a function that returns both the input and the target data for a given dataset\n",
    "\n",
    "Note:\n",
    "\n",
    "1. You can use `csv.reader()` to read the dataset, or rely on other methods such as `pandas`.\n",
    "2. For the wine dataset, subtract the target by `-1` to get the target values in range $\\{0, 1, 2\\}$.\n",
    "3. Be aware both datasets are sorted w.r.t. their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "\n",
    "def dataset(dataset_file=\"wine.data\"):\n",
    "    # read dataset\n",
    "    data = []\n",
    "    with open(dataset_file, 'r') as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        for row in reader:\n",
    "            data.append([float(val) for val in row])\n",
    "\n",
    "    print(f\"Loaded dataset with {len(data)} samples\")\n",
    "\n",
    "    # convert to torch.tensor\n",
    "    data = torch.tensor(data).float()\n",
    "\n",
    "    if dataset_file == \"wine.data\":\n",
    "        # target is in the first column and needs to be converted to long\n",
    "        X = data[:, 1:]\n",
    "        T = data[:, 0].long() - 1\n",
    "    else:\n",
    "        # target is in the last column and needs to be of type float\n",
    "        X = data[:, :-1]\n",
    "        T = data[:, -1].float()\n",
    "\n",
    "    return X, T\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Dataset Check\n",
    "\n",
    "Test 1 assures the correctness of the data and target dimensions.\n",
    "\n",
    "1. For the wine dataset, we make sure that the dataset is in the correct dimensions, i.e., $\\mathbf X\\in \\mathbb R^{N\\times D}$ and $\\mathbf T \\in \\mathbb N^N$. And all class labels are in the correct range $[0, O-1]$ where $O$ is the number of classes.\n",
    "\n",
    "2. For the spambase data, we assure that all dimensions are correct and that class labels are in range $\\{0, 1\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 178 samples\n",
      "Loaded dataset with 4601 samples\n"
     ]
    }
   ],
   "source": [
    "X, T = dataset(\"wine.data\")\n",
    "\n",
    "assert X.shape[1] == 13, X.shape[1]\n",
    "assert torch.all(T >= 0) and torch.all(T <= 2)\n",
    "assert T.dtype == torch.long\n",
    "\n",
    "X, T = dataset(\"spambase.data\")\n",
    "assert X.shape[1] == 57, X.shape[1]\n",
    "assert len(T) == X.shape[0], len(T)\n",
    "assert torch.all(T >= 0) and torch.all(T <= 1)\n",
    "assert T.dtype == torch.float"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Split Training and Validation Data\n",
    "\n",
    "The data should be split into 80% for training and 20% for validation. Implement a function that takes the full dataset $(X,T)$ and returns $(X_t, T_t, X_v, T_v)$ accordingly.\n",
    "\n",
    "Write a function that splits off training and validation samples from a given dataset. What do we need to assure before splitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_training_data(X,T,train_percentage=0.8):\n",
    "  \n",
    "  # split into 80/20 training/validation\n",
    "        \n",
    "    split = int(len(X)*train_percentage)\n",
    "    X_train = X[: split]\n",
    "    T_train = T[:split]\n",
    "    X_test =  X[split:]\n",
    "    T_test = T[split:]\n",
    "    \n",
    "    return X_train, T_train, X_test, T_test\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Input Data Standardization\n",
    "\n",
    "As we have seen last week, the standardization of the data provides many advantages. \n",
    "Hence, in this task you should write a function that takes $(X_t,X_v)$ as input and standardizes them by subtracting the mean and dividing by the \n",
    "standard deviation of $X_t$, and returning the standardized versions of both. Assure that each input dimension is standardized individually.\n",
    "\n",
    "Implement a function that standardizes all input data for the training and validation set.\n",
    "Return the standardized data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "1. Use `torch.mean()` and `torch.std()` with the correct dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(X_train, X_val):\n",
    "    # compute mean and standard deviation from X_train\n",
    "    mean = X_train.mean(dim=0)\n",
    "    std = X_train.std(dim=0)\n",
    "\n",
    "    # standardize both X_train and X_val\n",
    "    X_train_std = (X_train - mean) / std\n",
    "    X_val_std = (X_val - mean) / std\n",
    "\n",
    "    return X_train_std, X_val_std"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Implementation\n",
    "\n",
    "We will use a two-layer fully-connected network with $D$ input neurons, $K$ hidden neurons and $O$ output neurons. \n",
    "Depending on the task, $D$ and $O$ need to be selected appropriately, while $K$ is a parameter to play around with. \n",
    "In PyTorch, the easiest way to implement a network is by providing the requested sequence of layers to `torch.nn.Sequential`, which will build a network containing the given layers. \n",
    "We will use two `torch.nn.Linear` layers and one `torch.nn.Tanh` activation function in between. \n",
    "The network will return the logits $\\vec z$ for a given input $\\vec x$.\n",
    "\n",
    "\n",
    "### Task 4: Implement Network\n",
    "\n",
    "Implement a two-layer fully-connected network in PyTorch. \n",
    "The given network uses $\\tanh$ as activation function, and provide the possibility to change the number of inputs $D$, the number of hidden neurons $K$ and the number of outputs $O$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def Network(D, K, O):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(D, K),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(K, O)\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Accuracy Computation\n",
    "\n",
    "To monitor the training process, we want to compute the accuracy. \n",
    "The function will obtain the logits $\\vec z$ extracted from the network and the according target $t$. \n",
    "Assure that this function works both for binary and categorical classification. \n",
    "How can we identify, which of the two variants is currently required?\n",
    "\n",
    "Note: you can make use of the following pytorch functions:\n",
    "\n",
    "1. `torch.mean()` which computes the mean or average of the input tensor.\n",
    "2. `torch.argmax()` which returns the indices of the maximum values of all elements of the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def accuracy(Z, T):\n",
    "    # check if we have binary or categorical classification\n",
    "    if Z.shape[1] == 1:\n",
    "        # binary classification\n",
    "        preds = (Z >= 0).float()\n",
    "        correct = (preds == T).float()\n",
    "        return torch.mean(correct)\n",
    "    else:\n",
    "        # categorical classification\n",
    "        preds = torch.argmax(Z, dim=1)\n",
    "        correct = (preds == T).float()\n",
    "        return torch.mean(correct)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Test Accuracy Function\n",
    "\n",
    "Test 2 assures the correctness of your accuracy function in both binary and categorical cases. We make sure that the accuracy will compute the correct values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, test binary classification\n",
    "ZZ = torch.ones((20,1)) * -5.\n",
    "ZZ[15:20] = 5\n",
    "assert(abs(accuracy(ZZ,torch.zeros((20,1))) - 0.75) < 1e-8)\n",
    "assert(abs(accuracy(ZZ,torch.ones((20,1))) - 0.25) < 1e-8)\n",
    "\n",
    "# now, test categorical classification with 4 classes\n",
    "ZZ = torch.ones((20,4)) * -5\n",
    "ZZ[0:1,0] = 5\n",
    "ZZ[1:4,1] = 5\n",
    "ZZ[4:10,2] = 5\n",
    "ZZ[10:20,3] = 5\n",
    "\n",
    "assert(abs(accuracy(ZZ,torch.zeros(20)) - 0.05) < 1e-8)\n",
    "assert(abs(accuracy(ZZ,torch.ones(20)) - 0.15) < 1e-8)\n",
    "assert(abs(accuracy(ZZ,torch.ones(20)*2) - 0.3) < 1e-8)\n",
    "assert(abs(accuracy(ZZ,torch.ones(20)*3) - 0.5) < 1e-8)\n",
    "assert(abs(accuracy(ZZ,torch.tensor((0,1,1,1,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3))) - 1.) < 1e-8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Training Loop\n",
    "\n",
    "Implement a function that takes all necessary parameters to run a training on a given dataset.\n",
    "In this week, we will run gradient descent, i.e., we will train on the whole dataset in each training step, so there is no need to define anything related to batches. \n",
    "Select the optimizer to be `torch.optim.SGD`. \n",
    "\n",
    "Implement a training loop over 10'000 epochs with a learning rate of $\\eta=0.1$. \n",
    "Make sure that you train on the training data only, and **not** on the validation data.\n",
    "In each loop, compute and store the training loss, training accuracy, validation loss and validation accuracy. \n",
    "At the end, return the lists of these values.\n",
    "\n",
    "Note:\n",
    "\n",
    "1. When storing accuracy or loss values in a list, make sure to convert the to float via `v.item()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, T_train, X_val, T_val, network, loss, eta = 0.1, epochs = 10000):\n",
    "  optimizer = torch.optim.SGD(params=network.parameters(), lr = eta)\n",
    "\n",
    "  # collect loss and accuracy values\n",
    "  train_loss, train_acc, val_loss, val_acc = [], [], [], []\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    # train on training set\n",
    "    # compute network output on training data\n",
    "    optimizer.zero_grad()\n",
    "    Z = network(X_train)\n",
    "    \n",
    "    # compute loss from network output and target data\n",
    "    T_train = T_train.unsqueeze(1)  # add a new dimension\n",
    "    J = loss(Z, T_train)\n",
    "    J.backward() \n",
    "    \n",
    "    # perform parameter update\n",
    "    optimizer.step()\n",
    "    \n",
    "    # remember loss\n",
    "    train_loss.append(J.item())\n",
    "    \n",
    "    # compute training set accuracy\n",
    "    train_acc.append(accuracy(Z, T_train).item())\n",
    "\n",
    "    # test on validation data\n",
    "    with torch.no_grad():\n",
    "      # compute network output on validation data\n",
    "      Z = network(X_val)\n",
    "      # compute loss from network output and target data\n",
    "      J = loss(Z, T_val)\n",
    "      # remember loss\n",
    "      val_loss.append(J.item())\n",
    "      # compute validation set accuracy\n",
    "      val_acc.append(accuracy(Z, T_train).item())\n",
    "\n",
    "  # return the four lists of losses and accuracies\n",
    "  return train_loss, train_acc, val_loss, val_acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation\n",
    "\n",
    "Finally, we want to train our network on our data and plot the accuracy and loss values that were obtained through the epochs. \n",
    "Exemplary plots can be found in the exercise slides.\n",
    "\n",
    "\n",
    "### Task 7: Plotting Function\n",
    "\n",
    "Implement a function that takes four lists containing the training loss, the training accuracy, the validation loss and the validation accuracy and plot them into two plots. \n",
    "The first plot should contain the loss values for both training and validation. The second plot should contain the according accuracy values.\n",
    "\n",
    "Note:\n",
    "\n",
    "1. You might need to convert remaining `torch.tensor` values to `float`, lists, or `numpy.nadrray` for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "def plot(train_loss, train_acc, val_loss, val_acc):\n",
    "  pyplot.figure(figsize=(10,3))\n",
    "  ax = pyplot.subplot(121)\n",
    "  ax.plot(train_loss, \"g-\", label=\"Training set loss\")\n",
    "  ax.plot(val_loss, \"b-\", label=\"Validation set loss\")\n",
    "  ax.legend()\n",
    "\n",
    "  ax = pyplot.subplot(122)\n",
    "  ax.plot(train_acc, \"g-\", label=\"Training set accuracy\")\n",
    "  ax.plot(val_acc, \"b-\", label=\"Validation set accuracy\")\n",
    "  ax.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8: Binary Classification\n",
    "\n",
    "\n",
    "1. Load the data for binary classification, using the ``\"spambase.data\"`` file.\n",
    "2. Split the data into training and validation sets.\n",
    "3. Standardize both training and validation input data using the function from Task 3.\n",
    "4. Instantiate a network with the correct number of input neurons, a reasonable number of $K$ hidden neurons and one output neuron.\n",
    "\n",
    "Which loss function do we need for this task?\n",
    "\n",
    "Train the network with our data for 10'000 epochs and plot the training and validation accuracies and losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 4601 samples\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "all elements of input should be between 0 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_39854/634908170.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# train network on our data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# plot the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_39854/2482273765.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(X_train, T_train, X_val, T_val, network, loss, eta, epochs)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# compute loss from network output and target data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mT_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# add a new dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mJ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mJ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3096\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3098\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3099\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: all elements of input should be between 0 and 1"
     ]
    }
   ],
   "source": [
    "# define loss function\n",
    "loss = loss_fn = torch.nn.BCELoss() # Binary cross-entropy loss function\n",
    "# load dataset\n",
    "X, T = dataset(dataset_file=\"spambase.data\")\n",
    "# split dataset\n",
    "X_train, T_train, X_val, T_val = split_training_data(X,T)\n",
    "# standardize input data\n",
    "X_train, X_val = standardize(X_train, X_val)\n",
    "# instantiate network\n",
    "network = Network(57, 420, 1)\n",
    "\n",
    "# train network on our data\n",
    "\n",
    "results = train(X_train, T_train, X_val, T_val, network, loss)\n",
    "\n",
    "# plot the results\n",
    "plot(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9: Categorical Classification\n",
    "\n",
    "Perform the same tasks with the ``\"wine.data\"`` dataset for categorical classification. \n",
    "How many input and output neurons do we need?\n",
    "Change the number of input, hidden, and output neurons accordingly.\n",
    "\n",
    "Select the appropriate loss function for categorical classification.\n",
    "Which loss function will we need this time?\n",
    "\n",
    "How many hidden neurons will we need to get 100% training set accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 178 samples\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([142, 1])) that is different to the input size (torch.Size([142, 3])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m network \u001b[39m=\u001b[39m Network(\u001b[39m13\u001b[39m,\u001b[39m69\u001b[39m,\u001b[39m3\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[39m# train network on our data\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m results \u001b[39m=\u001b[39m train(X_train, T_train, X_val, T_val, network, loss)\n\u001b[0;32m     17\u001b[0m \u001b[39m# plot the results\u001b[39;00m\n\u001b[0;32m     18\u001b[0m plot(results)\n",
      "Cell \u001b[1;32mIn[37], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(X_train, T_train, X_val, T_val, network, loss, eta, epochs)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m# compute loss from network output and target data\u001b[39;00m\n\u001b[0;32m     14\u001b[0m T_train \u001b[39m=\u001b[39m T_train\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)  \u001b[39m# add a new dimension\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m J \u001b[39m=\u001b[39m loss(Z, T_train)\n\u001b[0;32m     16\u001b[0m J\u001b[39m.\u001b[39mbackward() \n\u001b[0;32m     18\u001b[0m \u001b[39m# perform parameter update\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:619\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 619\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:3089\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3087\u001b[0m     reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3088\u001b[0m \u001b[39mif\u001b[39;00m target\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize():\n\u001b[1;32m-> 3089\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   3090\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing a target size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) that is different to the input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) is deprecated. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3091\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease ensure they have the same size.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m   3092\u001b[0m     )\n\u001b[0;32m   3094\u001b[0m \u001b[39mif\u001b[39;00m weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3095\u001b[0m     new_size \u001b[39m=\u001b[39m _infer_size(target\u001b[39m.\u001b[39msize(), weight\u001b[39m.\u001b[39msize())\n",
      "\u001b[1;31mValueError\u001b[0m: Using a target size (torch.Size([142, 1])) that is different to the input size (torch.Size([142, 3])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# define loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# load dataset\n",
    "X, T = dataset(dataset_file=\"wine.data\")\n",
    "# split dataset\n",
    "X_train, T_train, X_val, T_val = split_training_data(X,T)\n",
    "# standardize input data\n",
    "X_train, X_val = standardize(X_train, X_val)\n",
    "# instantiate network\n",
    "network = Network(13,69,3)\n",
    "\n",
    "# train network on our data\n",
    "results = train(X_train, T_train, X_val, T_val, network, loss)\n",
    "\n",
    "# plot the results\n",
    "plot(results)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2dd53f8ad749bca69f7250ce75eb4f0def59db5cf79075a9716322ffc58e8a2e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('DL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
